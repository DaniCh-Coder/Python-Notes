{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text and prompting\n",
    "###### https://platform.openai.com/docs/guides/text\n",
    "Veamos como solicitar una interacción y respuesta de texto\n",
    "+ Con la API de OpenAI, se usar un LLM para generar texto a partir de una solicitud, como lo harías con ChatGPT . \n",
    "+ Los modelos pueden generar prácticamente cualquier tipo de respuesta\n",
    "  + textual, como código, ecuaciones matemáticas, datos JSON estructurados o prosa con un lenguaje similar al humano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Cargar variables de entorno desde .env\n",
    "load_dotenv()\n",
    "\n",
    "# Obtener la clave API desde el archivo .env\n",
    "OpenAI.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se muestra un ejemplo sencillo que utiliza la API de responses.\n",
    "###### https://platform.openai.com/docs/api-reference/responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Por qué los pájaros no usan Facebook? Porque ya tienen Twitter.\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=\"Cuentame un chiste en una sola frase\"\n",
    ")\n",
    "\n",
    "print(response.output_text)\n",
    "\n",
    "# Like: ¿Por qué los pájaros no usan Facebook? Porque ya tienen Twitter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El SDK oficial incluye una propiedad **output_text** propiedad en las respuestas del modelo para mayor comodidad, \n",
    "+ que agrega todos los resultados de texto del modelo en una sola cadena. \n",
    "+ Esto puede ser útil como acceso directo para acceder a los resultados de texto del modelo.\n",
    "\n",
    "Además de texto simple, también puede hacer que el modelo devuelva datos estructurados en formato JSON: esta característica se llama Salidas estructuradas .\n",
    "###### https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roles de mensajes y seguimiento de instrucciones\n",
    "Sepueden proporcionar instrucciones al modelo con diferentes niveles de autoridad utilizando el instructionsparámetro API o los roles de mensaje .\n",
    "\n",
    "**instructions** es im parámetro que proporciona al modelo instrucciones generales sobre cómo debe comportarse al generar una respuesta, incluyendo el tono, los objetivos y ejemplos de respuestas correctas. \n",
    "+ Cualquier instrucción proporcionada de esta manera tendrá prioridad sobre una solicitud en el parámetro **input**\n",
    "\n",
    "Con la API anterior **input** era una lista dónde se podía poner **role**. Ahora con **instructions** no es necesario.\n",
    "\n",
    "\n",
    "Por lo tanto la prioridad es:\n",
    "1. instructions\n",
    "2. input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr, ye be right in wonderin'! In JavaScript, semicolons be optional due to somethin' called automatic semicolon insertion. The language be tryin' to add 'em where it thinks they be needed. But beware, matey! It don't always get it right, and strange bugs might sail yer way. Best to use 'em wisely to avoid sinkin' yer code. Hoist the mainsail and code smartly, ye savvy?\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    instructions=\"Talk like a pirate.\",\n",
    "    input=\"Are semicolons optional in JavaScript?\",\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions versus developer messages in multi-turn conversations\n",
    "En el ejemplo anterior, **instruction=\"Talk like a pirate.\"** es equivalente al rol de developer.\n",
    "+ Es mas estilo chat GPT\n",
    "  \n",
    "Pero el rol *developer* no ha sido eliminado sino que aún *persiste*. \n",
    "(The OpenAI model spec describes how our models give different levels of priority to messages with different roles.)\n",
    "1. Los mensajes para desarrolladores son instrucciones proporcionadas por el desarrollador de la aplicación, que se anteponen a los mensajes del usuario.\n",
    "2. Los mensajes de usuario son instrucciones proporcionadas por un usuario final, que se incluyen detrás de los mensajes del desarrollador.\n",
    "3. Los mensajes generados por el modelo tienen el rol de asistente.\n",
    "\n",
    "Una conversación de varios turnos puede constar de varios mensajes de estos tipos, junto con otros tipos de contenido proporcionados tanto por el desarrollador, como el usuario y/o el modelo.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elección de un modelo\n",
    "Una decisión clave al generar contenido a través de la API es qué modelo utilizar: el parámetro \"model\" de los ejemplos de código anteriores. Puede encontrar una lista completa de los modelos disponibles aquí.\n",
    "\n",
    "¿Qué modelo debería elegir?\n",
    "A continuación, se presentan algunos factores a considerar al elegir un modelo para la generación de texto.\n",
    "\n",
    "+ *Reasoning Models*: Los modelos de razonamiento generan una cadena de pensamiento interna para analizar la solicitud de entrada y son excelentes para comprender tareas complejas y la planificación de varios pasos. \n",
    "  + Además, suelen ser más lentos y costosos de usar que los modelos GPT.\n",
    "  \n",
    "+ *GPT Models*: Los modelos GPT son rápidos, rentables y muy inteligentes, pero se benefician de instrucciones más explícitas sobre cómo realizar las tareas.\n",
    "\n",
    "+ *Mini Models*: Los modelos grandes y pequeños (mini) ofrecen ventajas competitivas en cuanto a velocidad, costo e inteligencia. \n",
    "  + Los modelos grandes son más eficaces para comprender las solicitudes y resolver problemas en diferentes dominios, mientras que los modelos pequeños suelen ser más rápidos y económicos de usar. \n",
    "  + Los modelos pequeños, como el GPT-4o mini, también pueden entrenarse para que destaquen en una tarea específica mediante el ajuste fino y la destilación de resultados de modelos más grandes. \n",
    "  + En caso de duda, gpt-4o ofrece una combinación sólida de inteligencia, velocidad y rentabilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingeniería de indicaciones\n",
    "Crear instrucciones eficaces para que un modelo genere contenido es un proceso conocido como ingeniería de indicaciones. (prompting engineering)\n",
    "+ Dado que el contenido generado a partir de un modelo no es determinista, crear una indicación que genere el tipo de contenido adecuado es una combinación de arte y ciencia. \n",
    "\n",
    "+ Aquí hay algunas pautas generales:\n",
    "    + Ser detallado en las instrucciones al modelo para eliminar la ambigüedad sobre cómo desea que responda.\n",
    "    + Proporcionar ejemplos al modelo del tipo de entradas que espera y del tipo de salidas que desea para esa entrada; \n",
    "      + esta técnica se denomina aprendizaje de pocos intentos.\n",
    "\n",
    "    + Al utilizar un modelo de razonamiento, describir la tarea a realizar en términos de objetivos y resultados deseados, \n",
    "      + en lugar de instrucciones específicas paso a paso sobre cómo realizar una tarea.\n",
    "\n",
    "  + Invertir en la creación de evaluaciones (evals) para sus indicaciones, utilizando datos de prueba que se asemejen a los datos que espera ver en producción. \n",
    "    + Debido a la variabilidad inherente de los resultados de los diferentes modelos, usar evaluaciones para comprobar el rendimiento de las indicaciones es la mejor manera de garantizar que funcionen como se espera.\n",
    "  + Iterar sobre las indicaciones suele ser suficiente para obtener excelentes resultados de un modelo, pero también se pueden realizar ajustes para personalizar los modelos base para un caso de uso específico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear una respuesta del modelo. Pedir o preguntarle al modelo.\n",
    "###### https://api.openai.com/v1/responses\n",
    "Se trata de proporcionar entradas de texto o imagen para generar salidas de texto o JSON. \n",
    "Tambien es posible hacer que el modelo llame a tu propio código personalizado o usa herramientas integradas, como\n",
    "+ la búsqueda web o de archivos, para usar tus propios datos como entrada para la respuesta del modelo.\n",
    "\n",
    "Cuerpo de la solicitud (Request Body)\n",
    "El input de un pedido al modelo es obligatorio. Este input puede contener:\n",
    "+ Entradas de texto, imagen o archivo al modelo, utilizadas para generar una respuesta. Es decir:\n",
    "    1. Entradas y salidas de texto\n",
    "    2. Entradas de imagen\n",
    "    3. Entradas de archivo\n",
    "    4. Estado de la conversación\n",
    "    5. Llamada a funciones\n",
    "\n",
    "Ver los ejemplos en este mismo folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Por dónde seguir:\n",
    "1. Prompting en Playground: https://platform.openai.com/playground/prompts\n",
    "2. JSON data with Structured Outputs: https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses\n",
    "3. Full API reference: https://platform.openai.com/docs/api-reference/responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
