{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large DataSets Scaling (LDS) - Load less data - Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some workloads can be achieved with chunking by splitting a large problem into a bunch of small problems. \n",
    "For example, converting an individual CSV file into a Parquet file and repeating that for each file in a directory. As long as each chunk fits in memory, you can work with datasets that are much larger than memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking works well when the operation you’re performing requires zero or minimal coordination between chunks. For more complicated workflows, you’re better off using another library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a chunked dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set a function to create data to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make a dataframe to work with scaling datasets\n",
    "def make_timeseries(start=\"2024-01-01\", end=\"2024-12-31\", freq=\"1D\", seed=None):\n",
    "    \"\"\"Build a dataset\"\"\"\n",
    "    # Build an index\n",
    "    index = pd.date_range(start=start, end=end, freq=freq, name=\"timestamp\")\n",
    "    n = len(index)\n",
    "    state = np.random.RandomState(seed)\n",
    "    columns = {\n",
    "        \"name\": state.choice([\"Alice\", \"Bob\", \"Charlie\"], size=n),\n",
    "        \"id\": state.poisson(1000, size=n),\n",
    "        \"x\": state.rand(n) * 2 - 1,\n",
    "        \"y\": state.rand(n) * 2 - 1,\n",
    "    }\n",
    "    df = pd.DataFrame(columns, index=index, columns=sorted(columns))\n",
    "    if df.index[-1] == end:\n",
    "        df = df.iloc[:-1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 12\n",
    "starts = [f\"20{i+10:>02d}-01-01\" for i in range(N)]\n",
    "ends = [f\"20{i+10:>02d}-12-31\" for i in range(N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathlib.Path(\"./timeseries\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (start, end) in enumerate(zip(starts, ends)):\n",
    "    ts = make_timeseries(start=start, end=end, freq=\"1min\", seed=i)\n",
    "    ts.to_parquet(f\"timeseries/ts-{i:0>2d}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is already builded and organized in 12 timeseries folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation of processing a Chunked File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've implemented an out-of-core pandas.Series.value_counts(). The peak memory usage of this workflow is the single largest chunk, plus a small series storing the unique value counts up to this point. As long as each individual file fits in memory, this will work for arbitrary-sized datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name\n",
       "Alice      2098234\n",
       "Bob        2097382\n",
       "Charlie    2098636\n",
       "dtype: int32"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = pathlib.Path(\"timeseries/\").glob(\"ts*.parquet\")\n",
    "counts = pd.Series(dtype=int)\n",
    "for path in files:\n",
    "    df = pd.read_parquet(path)\n",
    "    counts = counts.add(df[\"name\"].value_counts(), fill_value=0)\n",
    "counts.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some readers, like pandas.read_csv(), offer parameters to control the chunksize when reading a single file.\n",
    "\n",
    "Manually chunking is an OK option for workflows that don’t require too sophisticated of operations. Some operations, like pandas.DataFrame.groupby(), are much harder to do chunkwise. In these cases, you may be better switching to a different library that implements these out-of-core algorithms for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
